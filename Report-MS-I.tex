\section{Milestone I}
\subsection{Introduction}

\subsubsection{Enhancements to Project Structure and Codebase}
To streamline our model development process and enhance code readability and maintainability, we introduced a new Python module, \texttt{tuning.py}.
The \texttt{tuning.py} file encapsulates functions for conducting K-Fold cross-validation.

A 30\% validation set was included in main. From there we wanted to print different outputs if the \texttt{--test} was given or not. So we changed the lines that were computing accuracy's and F1 scores to make them clearer.

\subsection{Data Processing}
For ridge/linear regression and KNN the dataset was normalised before training. For ridge/linear regression and logistic regression the dataset was also modified by appending bias term before training.

\subsubsection{Normalization}
Normalization adjusts features to equalize their contributions by transforming each feature \(x\) using: \[ x_{\text{normalized}} = \frac{x - \mu}{\sigma} \] where \(\mu\) and \(\sigma\) are the feature's mean and standard deviation. This step prevents features with larger scales from dominating the model.

\subsubsection{Adding a Bias Term}
Integrating a bias term in linear regression and logistic regression accommodates non-origin patterns. By appending a ones column to \(X\), the model \(y = XW\) includes this term, and thus accounts for offsets.

\subsection{K-Fold Cross Validation}
To better tune our hyper parameters, we decided to use a 5-fold cross validation. That way, less data is lost compared to the standard validation set. The optimal parameter is chosen respectively for each method, by finding the parameter (from a large set of predifined values) which yields the highest validation metric chosen for the task and the model.

\subsection{Linear Regression}

\subsubsection{Method}
Ridge/linear regression model was trained on the pre-processed (normalised and appended with bias) dataset. It's performance was evaluated using the mean square error.

\subsubsection{Experiment and Results}
To find the optimal lamda, we used a 5 fold cross validation over the following values: $[ 10^{-3},10^{-2},10^{-1},10^{0},10^{1},10^{2},10^{3} ]$. The best cross validation MSE was given by lamda = 0.1: 
\begin{center}
  \begin{tabular}{l l l l l}
    \hline
    & \textbf{Training Set} & \textbf{5 Cross Validation Set} & \textbf{30\% Validation Set} & \textbf{Test Set}\\ 
    \hline 
    \textbf{MSE} & 0.005, & 0.005, & 0.006, & 0.005 \\
    \hline 
    \end{tabular}
    \captionof{table}{\textit{MSE for lmda = 0.1}}\label{tab:macro_f1_accuracy}
\end{center}

Also, interestingly by extending the range of possible values for lamda, we can observe that it follows the shape of a sigmoid function (\cref{fig:images}). That is, approximately, values for lambda $<$ 1000 are approximately at  0.005 and then for lamda $>$ $10^{5}$ are approximately at 0.26. 

\subsubsection{Discussion}
The MSE's for lamda = 0.1 are at a good level, also on the test set suggesting that our model has a certain capacity to capture the underlying relationship between the features and the target variable effectively. 


\subsection{Logistic Regression}


\subsubsection{Method}
For the logistic regression, the test labels were first converted to a one-hot encoded format,then the number of classes were extracted from the one-hot encoded array's dimensions.The feature count was determined from the test dataset's size. To optimize the model, a gradient loss function was computed to find the model's weights, which were initially set using a Gaussian distribution. This iterative process was repeated for a pre-defined number of iterations.

\subsubsection{Experiment and Results}
On the first try, with the proposed hyperparameters(1e-2,100) and without biais, we obtained an accuracy of 76\% for the training set(TS).
Then we tried to modify the way the weights where initalized by modifying the standart deviation but there were no significant result (0.5\% scope). However, by adding a bias, the accuracy jumped to 87.5\% for the TS. We also tried to add a polynomial expension of degree 2 to see if it would impact the perfomances(in utils). At a high number of iterations(10000), there was definitely an improvement on the accuracy as well as the F1 score of the TS so we decided to keep it (the bias is included in the function adding the polynomial). After this, we tried to find the best hyperparamaters using cross validation. From the observered results for k=5, we saw that the maximum average f1 score was between a lr of 1e-2 and 1.5e-2 with 300 iterations so we decided to use lr=1.3e-2,iters=300 (see annex below \cref{fig:images2}).It also confirmed the usefulness of the polynomial extension\\

\subsubsection{Discussion}
To conclude, althought we are already reaching a correct accuracy, there might be a bit of overfitting since there is a small difference of 4\% between the TS and the VS with a high number of iterations. To improve this, one solution would be to find a better feature expension than the polynomial one. We also noted that because of the softmax function using an exponential, 


%% Comment trouver les meilleurs param√®tres
\subsection{KNN}


\subsubsection{Method}
For the measure of distance, the euclidean distance was used. For the regression task, a simple average was used to compute the average label of the K nearest labels. As KNN doesn't really need any training, the fit method only stores the training data, labels and returns the prediction of the training data as suggested. The model is trained on pre-processed data, normalised.


\subsubsection{Experiment and Results}
To determine the optimal K, we used a 5 fold cross validation.

\subsubsection{Breed Identifying}
As accuracy doesn't take into account for classes imbalance, we decided to base our evaluation metric for the cross validation on the macro f1 score. Using it, we got \textbf{K = 7} (\cref{fig:images3}) and the following values:
\\

\begin{center}
  \begin{tabular}{l l l l l}
    \hline
    & \textbf{Training Set} & \textbf{5 Cross Validation Set} & \textbf{30\% Validation Set} & \textbf{Test Set}\\ 
    \hline 
    \textbf{Macro F1} & 0.879, & 0.865, & 0.859, & 0.864 \\
    \hline 
    \textbf{Accuracy} & 88.333\%, & 87.155\%, & 86.379\%, & 87.768\% \\
    \hline 
    \end{tabular}
    \captionof{table}{\textit{Macro F1 and Accuracy for K = 7}}\label{tab:macro_f1_accuracy}
\end{center}

\subsubsection{Center Locating}
Using the same approach as for breed identifying, we tried to find the best K with our 5 fold cross validation set. However, we found that K was inversely proportional to the MSE. Thus, as K increases, the MSE decreases.

\subsubsection{Discussion}
As we can see, the 5 cross validation set yields a better macro f1 and accuracy than the validation set, but also generalises better on the test set accuracy and macro f1. We would expect this to happened as the 5 cross fold looses less data than a standard validation set, but, more importantly, it also helps avoiding overfitting as it trains and then averages over multiple subsets of our training set. Finally we can see that the general trend for both f1 score and accuracy on the test set are at a good level, indicating that our model does have a certain capacity to generalize on unseen data.
Concerning regression tasks using our KNN model, we observe that the MSE is a decreasing function of K and converges to around 0.005. This indicates clearly that our model on this task does not generalise well, as a simple mean may give us a better approximation (when K grows higher, the KNN method will more and more converge towards a simple mean, as K will span nearly the entire space of points and average over it).


\newpage
\input{appendix.tex}
